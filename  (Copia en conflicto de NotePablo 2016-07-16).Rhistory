library(data.table)
library(mice)
library(VIM)
install.packages(VIM)
install.packages("VIM")
tp2.work <- read.csv("C:/Users/Pablo/Google Drive/Maestria/Aprendizaje Automático/TP 2", sep=";", stringsAsFactors=TRUE)
tp2.work <- read.csv("C:/Users/Pablo/Google Drive/Maestria/Aprendizaje Automático/TP 2/tp2-work.csv", sep=";", stringsAsFactors=TRUE)
library(data.table)
library(mice)
library(VIM)
DT_Train <- as.data.table(tp2.work)
DT_Train$anio <- factor(DT_Train$anio)
sum(!complete.cases(DT_Train)) # 13228 casos incompletos
md.pattern(DT_Train) #Faltantes en sup_cub_m2, cant_amb, sup_tot_m2, piso
table((DT_Train$Clase)) #5 Clases a predecir
aggr_plot <- aggr(DT_Train, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(DT_Train), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
install.packages("VIM")
library(VIM)
aggr_plot <- aggr(DT_Train, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(DT_Train), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
View(tp2.work)
library(tm)
install.packages(c("caret", "dendextend", "GGally", "httr", "jsonlite", "lazyeval", "phangorn", "plyr", "quantreg", "RWeka", "slam", "tidyr", "withr"))
library(XML)
test <- readHTMLTable("https://recursos-data.buenosaires.gob.ar/ckan2/ecobici/estado-ecobici.xml")
testdata <- xmlParse("https://recursos-data.buenosaires.gob.ar/ckan2/ecobici/estado-ecobici.xml")
library(RCurl)
install.packages(c("devtools", "dplyr", "GGally", "phytools", "ranger", "swirl", "xml2"))
tp2.work <- read.csv("C:/Users/Pablo/Google Drive/Maestria/Aprendizaje Automático/TP 2/tp2-work.csv", sep=";", stringsAsFactors=TRUE)
library(data.table)
library(mice)
library(VIM)
install.packages("VIM")
library(data.table)
library(mice)
library(VIM)
library(data.table)
library(mice)
library(VIM)
install.packages("quantreg")
library(data.table)
library(mice)
library(VIM)
library(tm)
library(wordcloud)
library(SnowballC)
tp2.work <- read.csv("C:/Users/Pablo/Google Drive/Maestria/Aprendizaje Automático/TP 2/tp2-work.csv", sep=";", stringsAsFactors=FALSE)
str(tp2.work)
review_text <- paste(tp2.work$tit, collapse=" ")
review_source <- VectorSource(review_text)
corpus <- Corpus(review_source)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
stopwords("spanish")
dtm <- DocumentTermMatrix(corpus)
dtm2 <- as.matrix(dtm)
frequency <- colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
head(frequency)
hist(frequency)
plot(frequency)
plot(frequency,row+)
plot(frequency,row.names(frequency))
plot(x=frequency,y=row.names(frequency))
install.packages("qdap")
freqs <- t(wfm(tp2.work$tit, 1:nrow(tp2.work)))
library(qdap)
freqs <- t(wfm(tp2.work$tit, 1:nrow(tp2.work)))
data.frame(tp2.work, freqs, check.names = FALSE)
ords <- rev(sort(colSums(freqs)))[1:9]      #top 9 words
top9 <- freqs[, names(ords)]                #grab those columns from freqs
data.frame(tp2.work, top9, check.names = FALSE) #put it together
tp2_train_v2 <- data.frame(tp2.work, top9, check.names = FALSE) #put it together
View(tp2_train_v2)
freqsdes <- t(wfm(tp2.work$des, 1:nrow(tp2.work)))
library(tm)
library(wordcloud)
library(SnowballC)
review_text <- paste(tp2.work$des, collapse=" ")
review_source <- VectorSource(review_text)
corpus <- Corpus(review_source)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
dtm <- DocumentTermMatrix(corpus)
dtm2 <- as.matrix(dtm)
frequency <- colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
head(frequency)
words <- names(frequency)
wordcloud(words[1:100], frequency[1:100])
dtmss <- removeSparseTerms(dtm, 0.15) # This makes a matrix that is only 15% empty space, maximum.
inspect(dtmss)
library(cluster)
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="ward")
fit
plot(fit, hang=-1)
freqsdes <- t(wfm(tp2.work$des[1:1000], 1:1000))
install.packages("qdap")
library(qdap)
freqsdes <- t(wfm(tp2.work$des[1:1000], 1:1000))
ords <- rev(sort(colSums(freqs)))[1:9]
ords <- rev(sort(colSums(freqsdes)))[1:9]
top9 <- freqsdes[, names(ords)]                #grab those columns from freqs
tp2_train_prueba <- data.frame(tp2.work, top9, check.names = FALSE) #put it together
View(tp2_train_prueba)
remove(d)
head(frequency)
review_text <- paste(tp2.work$des, collapse=" ")
review_source <- VectorSource(review_text)
corpus <- Corpus(review_source)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
dtm <- DocumentTermMatrix(corpus)
dtm2 <- as.matrix(dtm)
frequency <- colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
head(frequency)
dtm2
View(dtm2)
View(dtm2>3)
View(dtm2[(dtm2>3)==TRUE])
View(dtm2[(dtm2>3)==TRUE,])
View(dtm2[,(dtm2>3)==TRUE])
View(dtm2[(dtm2>3)==TRUE])
View(dtm2[(dtm2>3)])
View(dtm2[(dtm2>3),])
View(dtm2[>3,])
View(dtm2)
View(dtm2[,dtm2>3])
dtm2[,dtm2>3]
dtm2[dtm2>3]
dtm2[,dtm2>3]
ords <- rev(sort(colSums(dtm2)))[1:9]
top9 <- freqsdes[, names(ords)]                #grab those columns from freqs
top9
View(top9)
tp2_train_prueba <- data.frame(tp2.work, top9, check.names = FALSE) #put it together
txt <- "a test of capitalizing"
gsub("(\\w)(\\w*)", "\\U\\1\\L\\2", txt, perl=TRUE)
gsub("\\b(\\w)",    "\\U\\1",       txt, perl=TRUE)
grep("[a-z]", letters)
txt <- c("arm","foot","lefroo", "bafoobar")
if(length(i <- grep("foo", txt)))
cat("'foo' appears at least once in\n\t", txt, "\n")
i # 2 and 4
txt[i]
grep(tp2.work$tit,"amb")
View(tp2_train_v2)
grep(tp2.work$tit[3],"amb")
grep("amb",tp2.work$tit[3])
gsub("([ab])", "\\1_\\1_", "abc and ABC")
library(tm)
library(wordcloud)
library(SnowballC)
review_text <- paste(tp2.work$des, collapse=" ")
review_source <- VectorSource(review_text)
corpus <- Corpus(review_source)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
options(mc.cores=1)
library(RWeka)
library(rpart)
library(caret)
set.seed(123)
dtm <- DocumentTermMatrix(corpus)
dtm2 <- as.matrix(dtm)
mat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
mat.df <- cbind(mat.df, tp2.work$cant_amb)
View(mat.df)
colnames(mat.df)[ncol(mat.df)] <- "category"
train <- sample(nrow(mat.df$category[!is.na(mat.df$category)]), ceiling(nrow(mat.df$category[!is.na(mat.df$category) * .50))
train <- sample(nrow(mat.df$category[!is.na(mat.df$category)]), ceiling(nrow(mat.df$category[!is.na(mat.df$category) * .50)))
train <- sample(nrow(mat.df$category[!is.na(mat.df$category)]), ceiling(nrow(mat.df$category[!is.na(mat.df$category)] * .50)))
subset <- mat.df[!is.na(mat.df$category),]
subset$category <- factor(subset$category)
train <- sample(nrow(subset), ceiling(nrow(subset) * .50))
test <- (1:nrow(subset))[- train]
cl <- subset[, "category"]
modeldata <- subset[,!colnames(subset) %in% "category"]
knn.pred <- knn(modeldata[train, ], modeldata[test, ], cl[train])
library(class) # KNN model
set.seed(100)
train <- sample(nrow(subset), ceiling(nrow(subset) * .50))
test <- (1:nrow(subset))[- train]
cl <- subset[, "category"]
modeldata <- subset[,!colnames(subset) %in% "category"]
knn.pred <- knn(modeldata[train, ], modeldata[test, ], cl[train])
remove(freqs)
remove(freqsdes)
knn.pred <- knn(modeldata[train, ], modeldata[test, ], cl[train])
remove(mat.df)
remove(modeldata)
remove(subset)
library(tm)
library(wordcloud)
library(SnowballC)
library(RWeka)
library(rpart)
library(caret)
library(class) # KNN model
ords <- rev(sort(colSums(dtm2)))[1:9]
top9 <- dtm2[, names(ords)]
SE) #put it together
tp2_train_prueba <- data.frame(tp2.work, top9, check.names = FALSE) #put it together
frequency <- colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
ords <- rev(sort(colSums(dtm2)))[1:9]
top9 <- frequency[, names(ords)]
ords <- rev(sort(colSums(dtm2)))[1:9]
top9 <- frequency[, names(ords)]
ords
top9 <- frequency[names(ords),]
top9 <- frequency[,names(ords)]
head(frequency)
frequency[,1:10]
frequency[1:10]
top9 <- frequency[names(ords)]
tp2_train_prueba <- data.frame(tp2.work, top9, check.names = FALSE) #put it together
head(dtm2)
dtm3○ <- data.matrix(dtm)
dtm3 <- data.matrix(dtm)
head(dtm3)
mat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
head(mat.df)
head(mat.df[ords])
head(mat.df[,ords)
head(mat.df[,ords])
ords
head(mat.df[,names(ords)])
mat.df <- cbind(mat.df[,names(ords)], tp2.work$cant_amb)
View(mat.df)
colnames(mat.df)[ncol(mat.df)] <- "category"
head(review_text)
review_source
library(tm)
library(wordcloud)
library(SnowballC)
library(RWeka)
library(rpart)
library(caret)
library(class)
review_text <- paste(tp2.work$des, collapse=" ")
review_source <- VectorSource(review_text)
corpus <- Corpus(review_source)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
dtm <- DocumentTermMatrix(corpus)
dtm2 <- as.matrix(dtm)
View(dtm2)
remove(dtmss)
library(qdap)
frequency <- colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
ords <- rev(sort(colSums(dtm2)))[1:25]
freqs <- t(wfm(tp2.work$des, 1:nrow(tp2.work)))
library(tm)
library(wordcloud)
library(SnowballC)
library(RWeka)
library(rpart)
library(caret)
library(class)
frequency <- colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
ords <- rev(sort(colSums(dtm2)))[1:25]
top9 <- frequency[names(ords)]
tp2_train_prueba <- data.frame(tp2.work, top9, check.names = FALSE) #put it together
View(tp2_train_prueba)
remove(tp2_train_prueba)
mat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
mat.df <- cbind(mat.df[,names(ords)], tp2.work$cant_amb)
View(mat.df)
View(dtm2)
freqs <- t(wfm(tp2.work$tit, 1:nrow(tp2.work)))
library(qdap)
freqs <- t(wfm(tp2.work$tit, 1:nrow(tp2.work)))
data.frame(tp2.work, freqs, check.names = FALSE)
ords <- rev(sort(colSums(freqs)))[1:9]      #top 9 words
top9 <- freqs[, names(ords)]                #grab those columns from freqs
tp2_train_v2 <- data.frame(tp2.work, top9, check.names = FALSE) #put it together
View(tp2_train_v2)
remove(c("dtm2","dtm3","mat.df","cl","dtmss","fit","test","train","txt","words"))
remove(c('dtm2',"dtm3","mat.df","cl","dtmss","fit","test","train","txt","words"))
remove(c('dtm2',"dtm3","mat.df","cl","dtmss","fit","test","train","txt","words"))
remove(dtm2)
remove(dtm3)
remove(dtmss)
remove(mat.df)
remove(cl)
remove(fit)
remove(test)
remove(train)
remove(freqs)
remove(words)
remove(txt)
remove(dtm)
remove(frequency)
remove(i)
remove(top9)
library(qdap)
freqs <- t(wfm(tp2.work$des, 1:nrow(tp2.work)))
library(tm)
library(wordcloud)
library(SnowballC)
library(RWeka)
library(rpart)
library(caret)
library(class) # KNN model
review_text <- paste(tp2.work$des, collapse=" ")
review_text <- paste(tp2.work$des, collapse="")
review_source <- VectorSource(review_text)
corpus <- Corpus(review_source)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
dtm <- DocumentTermMatrix(corpus)
mat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
dtm2 <- as.matrix(dtm)
frequency <- colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
ords <- rev(sort(colSums(dtm2)))[1:25]
mat.df <- cbind(mat.df[,names(ords)], tp2.work$cant_amb)
View(mat.df)
knn <- read.csv("C:/Users/Pablo/Dropbox/TP 2 ML/knn.csv", sep=";", stringsAsFactors=FALSE)
View(knn)
set.seed(100)
# Packages
library(tm) # Text mining: Corpus and Document Term Matrix
library(class) # KNN model
library(SnowballC) # Stemming words
# Read csv with two columns: text and category
df <- read.csv("knn.csv", sep =";", header = TRUE)
# Create corpus
docs <- Corpus(VectorSource(df$Text))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument, language = "english")
# Create dtm
dtm <- DocumentTermMatrix(docs)
# Transform dtm to matrix to data frame - df is easier to work with
mat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
View(mat.df)
mat.df <- cbind(mat.df[,colSums(mat.df)>3], df$Category)
View(mat.df)
corpus <- Corpus(VectorSource(tp2.work$des))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("spanish"))
corpus <- tm_map(corpus, stemDocument, language = "spanish")
dtm <- DocumentTermMatrix(corpus)
mat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
remove(dtm2)
remove(knn)
remove(tp2_train_v2)
remove(frequency)
remove(ords)
remove(review_source)
remove(review_text)
remove(corpus)
mat.df <- as.data.frame(data.matrix(dtm), stringsAsfactors = FALSE)
1.813*9.7+2.2*0.364-9.396
0.777*9.7+2.2*1.296-5.876
